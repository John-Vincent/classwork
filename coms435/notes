Book
Mining of Massive Data Sets -- Rajaraman & Ullman & Leskovic

--8/21/18--

most algorithms are based on three probabilistic hypothetical
  1) toss a coin x times
  2) roll a dice x times
  3) pick a random element from set x

the sample space is the set of possible outcomes the sample point is a specific
observed outcome.

any subset of the sample space omega is called an Event E


--8/30/18--

Markov inequality 
    P[x>v] <= e[x]/v
    p[x>alpha e[x]] <= 1/alpha

chebyshev inequality
    p[|x-e[x]| >= delta] <= var(x) / delta^2

chernoffs bound
    let x_1 .... x_m be independent random variables
    that take values [0,1] and E[x_1]=E[x_2] = e[X_m] = p
    p[|x/m - p| > p delta] <= 2e^(-delta^2mp/2)
    e[x/m] = p

exp
    toss a fair coin n times
        p[we see > 2n/3 heads]
        e[x] = n/2
        variance(x) = sum 1 to n (var(x_i))
        var(x_i) = (1-.5)^2 * 1/2 + (0-.5)^2 * 1/2 = 1/4
        var(x) = sum 1 to n (var(x_i)) = n/4

        with Markov bound
            p[ x > 2n/3 ] <= (n/2)/(2n/3) = 3/4

        with chebyshev inequality
            p [ | x - e(x) | >= delta ] <= var(x)/delta^2
            p [ x-n/2 > n/6 ] 
            p [ x > 2n/3 ] <= p[ | x-n/2 | >= n/6 ] <= (n/4) / ( n^2 / 36 ) = 9/n

        with chernoffs bound
            p[x > 2n/3 ] = p[ x/2 > 2/3 ] <= p[ | x/2 -1/2 | >= 1/6 ] = p[ |x/n - 1/2| >= 1/2 * 1/3 ] <= 2e^(-1/9 * n * 1/2 * 1/2) = 2e^(-n/36) 

--9/6/18--
 hashing 
    maps universe to a finite set {1...n}
    smallest collisions possible is ideal
    random
        pick first prime p at least n.
        randomly pick a and b in {0...p-1}
        h_{ab} = (ax+b)%p (mod p ensures the function is 1 to 1 since it is prime)
    deterministic
        murmur
        SHS
        Jenkins
        fnv
            strings -> 64 bit
            p1: 64 bit prime
            p2: 64 bit prime
            h = p1
            for i = 0 to s.length -1
                    h = h xor s[i]
                    h = (h*p2)%2^{64}
    if the data is very structured then you have to use random hash functions for 
    this probabilistic data structures, if the data is very random than you can
    use a deterministic hashing function


 Bloom filter 
    probabilistic data structure that allows for compressed data sets that are operable.
    uses k random hash functions

 lost a bunch of notes due to aspell
 
--9/18/18--
count min sketch
    a probabilistic data structure
    used for finding most frequent pairs in a multi-set

    we want to store the frequency of every item in the set.
        t_x = frequency of x in the set
        let s be a multi-set of size n. An approximate q-heavy hitter
        {x| t_x >= qn}ut
        q = 1/10
        cms will output a set containing every x where t_x >= qn, every x where t_x <= qn/2
            9n/2 < ans < qn
        pr [^t_x >= t_x + en ] <= delta
        pr [^t_x < tx + en] >= 1-delta
        q = 2e
        approximate heavy hitter = { x | ^t_c >=2en}
